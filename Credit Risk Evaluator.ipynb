{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for training data\n",
    "X = train_df.drop('loan_status', axis=1)\n",
    "X_train = pd.get_dummies(X)\n",
    "y = LabelEncoder().fit_transform(train_df['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for testing data\n",
    "X2 = test_df.drop('loan_status', axis=1)\n",
    "X_test = pd.get_dummies(X2)\n",
    "y2 = LabelEncoder().fit_transform(test_df['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing dummy variables to testing set\n",
    "for x in X_train.columns:\n",
    "    if x not in X_test.columns:\n",
    "        X_test[x] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unscaled Data Prediction\n",
    "Logistic regression Vs. Random Forest \n",
    "\n",
    "I predict that the random forest classifier will perform better than the logistic regression model, because there are so many features in the data. The random forest classifier will provide greater opportunity to predict the outcome correctly, based on the features, compared to the logistic regression. Also, since the data is not scaled, the logistic regression model might not perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.6811986863711001\n",
      "Testing Data Score: 0.5542322415993194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annaweeks/opt/anaconda3/envs/PythonData/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:765: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Train the LOGISTIC REGRESSION model on the unscaled data and print the model score\n",
    "classifier = LogisticRegression(max_iter=1000).fit(X_train, y)\n",
    "\n",
    "print(f\"Training Data Score: {classifier.score(X_train, y)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6210123351765207\n"
     ]
    }
   ],
   "source": [
    "# Train a RANDOM FOREST Classifier model and print the model score\n",
    "# i.e. 70 % training dataset and 30 % test datasets\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=200).fit(X_train, y)\n",
    "\n",
    "print(f'Training Score: {clf.score(X_train, y)}')\n",
    "print(f'Testing Score: {clf.score(X_test, y2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unscaled Data Result\n",
    "\n",
    "The random forest classifier performed better than the logistic regression as I had anticipated. However, the random forest classifier model is overfitted to the training data and may not be the best representation for the 2020 test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaled Data Prediction\n",
    "I predict that scaling the data will improve the score for the logistic regression model since the scale for each feature will be more standard compared to the model. Scaling the data likely won't affect the random forest classifier model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.7127257799671592\n",
      "Testing Data Score: 0.7201190982560612\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the scaled data and print the model score\n",
    "classifier_scaled = LogisticRegression(max_iter=1000).fit(X_train_scaled, y) \n",
    "print(f\"Training Data Score: {classifier_scaled.score(X_train_scaled, y)}\")\n",
    "print(f\"Testing Data Score: {classifier_scaled.score(X_test_scaled, y2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6214376860910251\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "clf_scaled = RandomForestClassifier(random_state=1, n_estimators=200).fit(X_train_scaled, y)\n",
    "print(f'Training Score: {clf_scaled.score(X_train_scaled, y)}')\n",
    "print(f'Testing Score: {clf_scaled.score(X_test_scaled, y2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Findings\n",
    "\n",
    "As anticipated, scaling the data improved the results for the logistic regression, and the random forest classifier model does not seem affected. Additionally, the logistic regression performs better for the scaled data than random forest classifier model.\n",
    "\n",
    "Therefore, scaling the data and using a logistic regression model would be best the best approach out of the 4 attempted here. The data did not seem overfitted, and this approach gave the highest r-squared of approximately 0.720119, on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
